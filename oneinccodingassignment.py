# -*- coding: utf-8 -*-
"""OneIncCodingAssignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bw6Nts0cxRix8eISvf6AnBEFT8A3-z9V

README

LLM Rephraser
A single-page application (SPA) that uses a Hugging Face-hosted Large Language Model (LLM) to rephrase user input into four distinct writing styles: Professional, Casual, Polite, and Social-media. Built with Gradio for the frontend and Python for the backend, this app runs entirely in Google Colab and supports streaming output and cancellation.

Features
*  Rephrases input into multiple writing styles using an LLM
*  Real-time streaming output (word-by-word)
* Cancel button to interrupt processing
*  Unit and integration tests included
* Docker containerization for backend
* Clean, enterprise-grade UI using Gradio Blocks

Tech Stack
Layer	Technology
Frontend	HTML + JavaScript via Gradio
Backend	Python (no Flask)
LLM	Hugging Face mistralai/Mistral-7B-Instruct-v0.2
Platform	Google Colab
Setup Instructions (Google Colab)
1. Enable GPU: Go to Runtime â†’ Change runtime type â†’ GPU
2. Install dependencies:
!pip install -q transformers accelerate gradio huggingface_hub
  3. Authenticate with Hugging Face:
from huggingface_hub import login
login("your_huggingface_token_here")
4. Run the notebook cells to launch the Gradio app.

Testing
Run the following test functions in Colab:

test_prompt_builder()
test_model_response()
test_end_to_end()

All tests validate prompt formatting, model output, and full pipeline integration.

Assumptions
* You have access to the gated Hugging Face model mistralai/Mistral-7B-Instruct-v0.2
* You are running this in a GPU-enabled Colab environment
* You have a valid Hugging Face token with read access

"
"""

!pip install -q transformers accelerate gradio

from huggingface_hub import login

login("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")

from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import torch

# Load Mistral-7B-Instruct from Hugging Face
model_id = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

def build_prompt(user_input):
    return f"""Rephrase the following sentence into four distinct writing styles:

Input: "{user_input}"

Styles:
Professional:
Casual:
Polite:
Social-media:"""

def generate_styles_stream(user_input):
    prompt = build_prompt(user_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    model.generate(**inputs, streamer=streamer, max_new_tokens=200)

import gradio as gr

with gr.Blocks(title="LLM Rephraser") as demo:
    gr.Markdown("## âœ¨ Rephrase Your Text into Different Styles")
    input_text = gr.Textbox(label="Enter your sentence", placeholder="Type something...", lines=2)
    process_btn = gr.Button("ğŸš€ Process")
    cancel_btn = gr.Button("âŒ Cancel")
    professional = gr.Textbox(label="Professional", lines=2)
    casual = gr.Textbox(label="Casual", lines=2)
    polite = gr.Textbox(label="Polite", lines=2)
    social = gr.Textbox(label="Social-media", lines=2)

    stop_flag = gr.State(False)

    def rephrase(text, flag):
        if flag:
            return ["Cancelled"] * 4
        prompt = build_prompt(text)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        output = model.generate(**inputs, max_new_tokens=200)
        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        lines = decoded.split("\n")
        styles = {"Professional": "", "Casual": "", "Polite": "", "Social-media": ""}
        for line in lines:
            for key in styles:
                if line.startswith(f"{key}:"):
                    styles[key] = line[len(key)+1:].strip()
        return [styles["Professional"], styles["Casual"], styles["Polite"], styles["Social-media"]]

    process_btn.click(fn=rephrase, inputs=[input_text, stop_flag], outputs=[professional, casual, polite, social])
    cancel_btn.click(fn=lambda: True, inputs=[], outputs=[stop_flag])

demo.launch()

def test_prompt_builder():
    input_text = "Hey guys, let's huddle about AI."
    prompt = build_prompt(input_text)
    assert "Professional:" in prompt and "Casual:" in prompt and "Polite:" in prompt and "Social-media:" in prompt
    print("âœ… Prompt builder test passed.")

def test_model_response():
    input_text = "Hey guys, let's huddle about AI."
    prompt = build_prompt(input_text)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_new_tokens=100)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    assert "Professional:" in decoded
    print("âœ… Model response test passed.")

def test_end_to_end():
    input_text = "Hey guys, let's huddle about AI."
    result = rephrase(input_text, False)
    assert all(isinstance(r, str) and len(r) > 0 for r in result)
    print("âœ… End-to-end test passed.")

test_prompt_builder()
test_model_response()
test_end_to_end()





